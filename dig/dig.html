<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<title>DIG | Ren Li</title>
<meta name="generator" content="Jekyll v3.9.2">
<meta property="og:title" content="DIG">
<meta property="og:locale" content="en_US">
<meta name="description" content="DIG: Draping Implicit Garment over the Human Body">
<meta property="og:description" content="DIG: Draping Implicit Garment over the Human Body">
<meta property="og:site_name" content="DIG | Ren Li">
<meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="DIG">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"DIG: Draping Implicit Garment over the Human Body","headline":"DIG"}</script>

<link rel="stylesheet" href="dig_files/main.css">
<link rel="stylesheet" href="dig_files/dig.css">
<link rel="stylesheet" href="dig_files/academicons.min.css">

</head>

  <body>

    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <h1 style="text-align: center;">DIG: Draping Implicit Garment over the Human Body</h1>

<h2 style="text-align: center;">
    <a class="page-link" href="https://liren2515.github.io/page/">Ren Li</a><sup>1</sup>,
    <a class="page-link" href="https://bguillard.github.io/">Benoit Guillard</a><sup>1</sup>,
    <a class="page-link" href="https://scholar.google.com/citations?hl=en&user=yz2P_aUAAAAJ">Edoardo Remelli</a><sup>2</sup>,
    <a class="page-link" href="https://people.epfl.ch/pascal.fua?lang=en">Pascal Fua</a><sup>1</sup>
</h2>

<h3 style="text-align: center;"> 
  <div class="centered_div big">
    <sup>1</sup> CvLab, EPFL &emsp; <sup>2</sup> Meta Reality Labs Zürich
  </div>
</h3>

<div class="centered_div big">
    <div class="div_sidebyside"><img src="dig_files/epfl_logo.png"></div>
    <div class="div_sidebyside">
    	<a class="page-link" href="https://www.epfl.ch/labs/cvlab/">
    	<img src="dig_files/cvlab_logo.png"></a></div>
</div>

<div class="centered_div big" style="padding-bottom:20px;">
    <div class="div_rounded_corners"><a href="https://arxiv.org/abs/2209.10845" style="color: #fdfdfd;">
        <i class="ai ai-arxiv"></i> Paper
    </a></div>
    <div class="div_rounded_corners"><p style="color: #fdfdfd;"><object data="dig_files/github_logo.svg"></object>
        <a href="https://github.com/liren2515/DIG" style="color: #fdfdfd;">Code</a>
    </p></div>
</div>

<!-- Video will go here 
<div class="centered_div big">
 <iframe width="800" height="450" src="https://www.youtube.com/embed/TVMwnazd9lQ" frameborder="0"></iframe> 
</div>-->

<div class="div_text">
	<h1 style="text-align: center;">Abstract</h1>
	Existing data-driven methods for draping garments over human bodies, despite being effective, cannot handle garments of arbitrary topology and are typically not end-to-end differentiable. To address these limitations, we propose an end-to-end differentiable pipeline that represents garments using implicit surfaces and learns a skinning field conditioned on shape and pose parameters of an articulated body model. To limit body-garment interpenetrations and artifacts, we propose an interpenetration-aware pre-processing strategy of training data and a novel training loss that penalizes self-intersections while draping garments. We demonstrate that our method yields more accurate results for garment reconstruction and deformation with respect to state of the art methods. Furthermore, we show that our method, thanks to its end-to-end differentiability, allows to recover body and garments parameters jointly from image observations, something that previous work could not do. 
</div>

<hr class="hr_style">


<div class="div_text div_gray">
  <h3> Approach </h3>
  <div style="width: 100%; display:block; margin:auto; padding-bottom:2px;"><img style="margin:5px; border-radius:10px;" src="dig_files/pipeline.png" /></div>
  <div style="width: 100%; display:inline-block;">
    A garment in the canonical space is first reconstructed with an SDF auto-decoder [<i>DeepSDF</i>] from its latent code <b>z</b>. Then, given the shape <b>β</b> and pose <b>θ</b> of the target body, we predict and add the shape and pose displacements (<b>∆x<sub>β</sub></b> and <b>∆x<sub>θ</sub></b>) to the reconstructed garment and drape it to the target body with the skinning function of [<i>SMPL</i>].
  </div>
</div>

<div class="div_text div_gray">
  <div class="div_aligned">
    <div style="width: 42%; display:inline-block;"> 
      <h3 style="text-align: left;"> Garment Reconstruction </h3>
      Reconstructing garments as the 0-levelset of an SDF network yields inflated shaped that have some thickness. To prevent inflated garments from penetrating inside the body, we introduce a new pre-processing stage of the training set. We simply push garments vertices that are within the body outside of it.
    </div>
    <div style="width: 55%; display:inline-block; vertical-align:middle; margin-left:3%;"><img style="margin:5px; width:100%;border-radius:10px;" src="dig_files/garments_preprocessing.png" /> 
      <p class="fig_caption" style="width: 100%; padding-bottom:10px"> <i>Left:</i> standard inflated garment representation. <br><i>Right:</i> with our pre-processing. </p>
    </div> 
  </div>
</div>

<div class="div_text div_gray">
  <div class="div_aligned">
    <div style="width: 47%; display:inline-block; vertical-align:middle; margin-right:3%;"><img style="margin:5px; width:100%;border-radius:10px;" src="dig_files/deformations.png" />  <p class="fig_caption" style="width: 100%; padding-bottom:10px"> When deforming an inflated garment, such self intersections can happen. We add new loss terms to prevent them.
    </div>
    <div style="width: 50%; display:inline-block;"> 
      <h3 style="text-align: right;"> Garment Deformation </h3>
      To drape garments on a body, we predict vertex displacements conditioned on shape and pose. We extend SMPL's template body skinning function to the entire 3D space.
      <br>
      Our skinning weights and spatial deformations are learned with fully connected networks, from a dataset of simulated drapings [<i>CLOTH3D</i>].
      New loss terms prevent self-intersections and collisions with the body.
      <br>
      The resulting skinning function can be queried for garments of any topology, and is fully differentiable.
    </div>
  </div>
</div>


<div class="div_text div_gray">
    <h3> Fitting images </h3>
    <div style="width: 100%; display:block; margin:auto; padding-bottom:2px;"><img style="margin:5px; border-radius:10px;" src="dig_files/results.jpg" /></div>
    <p class="fig_caption" style="width: 100%; padding-bottom:10px"> Input &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Segmentation &emsp;&emsp;&emsp;&emsp; Reconstruction &emsp;&emsp;&emsp;&emsp; Side view &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Overlay </p>
    <div style="width: 100%; display:inline-block;">
      Our pipeline can be used to recover the body and garment shapes of clothed people from images. Since it is end-to-end differentiable, we can simply minimize a reprojection loss with respect to <b>β</b>, <b>θ</b> (body shape and pose), and <b>z</b> (garment latent code) by means of gradient descent. More specifically, given an input image, we first predict a segmentation mask and fit the output of our pipeline to it with a differentiable renderer.


    </div>
</div>


<h2> BibTeX </h2>
If you find our work useful, please cite it as:

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{ren2022dig,
  author = {Ren, Li and Guillard, Benoit and Remelli, Edoardo and Fua, Pascal},
  title = {{DIG: Draping Implicit Garment over the Human Body}},
  booktitle = {Asian Conference on Computer Vision},
  year = {2022}
}
</code></pre></div></div>


<hr class="hr_style_thin">

<h3> References </h3>
<div class="div_refs">
  <div class="div_aligned_top">
    <div style="width: 12%; display:inline-block; text-align:right; margin-right:1%;">[<i>DeepSDF</i>]</div>
    <div style="width: 87%; display:inline-block;">J. J. Park, P. Florence, J. Straub, R. A. Newcombe, and S. Lovegrove. DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation. In <i>Conference on Computer Vision and Pattern Recognition</i>, 2019.</div>
  </div>
</div>
<div class="div_refs">
  <div class="div_aligned_top">
    <div style="width: 12%; display:inline-block; text-align:right; margin-right:1%;">[<i>SMPL</i>]</div>
    <div style="width: 87%; display:inline-block;">M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, M. Black. SMPL: A skinned multi-person linear model. In <i>ACM transactions on graphics (TOG)</i>, 2015.</div>
  </div>
</div>
<div class="div_refs">
  <div class="div_aligned_top">
    <div style="width: 12%; display:inline-block; text-align:right; margin-right:1%;">[<i>CLOTH3D</i>]</div>
    <div style="width: 87%; display:inline-block;">H. Bertiche, M. Madadi, S. Escalera. CLOTH3D: Clothed 3D Humans. In: <i>European Conference on Computer Vision</i>, 2020.</div>
  </div>
</div>




</div></main></body></html>